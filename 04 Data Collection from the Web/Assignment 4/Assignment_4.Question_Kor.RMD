---
title: "assignment3"
output: html_document
---

# ??? 부분이 빈칸입니다.

# Question 1
#### 1-1 ) twitter api 계정 생성 후 빈칸을 채우시오.
```{r 1-1}
consumer_key= ???
consumer_secret= ???
access_token = ???
access_secret = ???
```

# Question 1
#### 1-2 ) 본인의 key를 이용하여 authentication을 완료하시오.
```{r 1-2}
???(consumer_key, consumer_secret, access_token, access_secret)
```

# Question 1
#### 1-3 ) @Google 계정의 트윗 100개를 가져온 후 10개를 출력하시오.
```{r 1-3}
GoogleTweets <- ???(???)
```

# Question 1
#### 1-4 ) 과제 제출일 기준 최근 7일간 hashtag #Yolo에 대한 트윗 100개를 가져온 후 10개를 출력하시오. 
```{r 1-4}
YoloTweets  <- ???(???)
```


# Question 2
#### 2-1 ) naver api 계정 생성 후 빈칸을 채우시오.
(계정 생성 시, 사용 API :검색, api 서비스 환경 :WEB, 주소 : http://127.0.0.1/ 로 지정할 것)
```{r 2-1}
client_id     = ???
client_secret = ???
```

# Question 2
#### 2-2 ) 검색 query를 전송하기 위하여 "프로그래밍 언어"를 퍼센트 인코딩 시키시오.
```{r 2-2}
query = ???(iconv("프로그래밍 언어", to="UTF-8"))
query = str_c("?query=", ???)
```

# Question 2
#### 2-3 ) GET()을 통하여 결과 데이터를 얻으시오(xml format).
```{r 2-3}
api_url = "https://openapi.naver.com/v1/search/webkr.xml"
result = GET(str_c(???, ???), 
             ???("X-Naver-Client-Id" = ???, "X-Naver-Client-Secret" = ???))

```

# Question 2
#### 2-4 ) 2-3에서 뽑은 결과물을(xml 형태) parsing한 후, title, link, description으로 나누어 저장 하시오.
```{r 2-4}
xml_ = ???(result)

titles <- ???(???, "/rss/channel/item/title", xmlValue)
links <- ???(???, "/rss/channel/item/link", xmlValue)
contents <- ???(???, "/rss/channel/item/description", xmlValue)
```



# Question 3  
#### 3 ) arxiv에서 "deep learning"으로 검색 후, 1,2쪽에 있는 논문 50편에 대하여 (subheader, title, author, subject, dateline)을 scraping하시오. 최종 결과물을 csv파일로 저장하시오.
```{r}

for( i in c(???)){
  
  tmp_url <- modify_url(url, query = list(skip = i))
  tmp_list <- read_html(tmp_url) %>% html_nodes('div#dlpage') %>% html_nodes('a[title="Abstract"]') %>% html_attr('href')
  tmp_list <- paste0('https://arxiv.org',tmp_list)
  
  for(j in 1:length(tmp_list)){
    
    tmp_paragraph <- ???(tmp_list[j])
    
    # subHeader
    tmp_subHeader <- tmp_paragraph %>% html_nodes(???) %>% html_text(T)
    subHeader <- c(subHeader, tmp_subHeader)
    
    # title
    tmp_title <- gsub('Title:\n', '',tmp_paragraph %>% html_nodes(???) %>% html_text(T))
    title <- c(title, tmp_title)
    
    # author
    tmp_author <- tmp_paragraph %>% html_nodes(???) %>% html_text
    tmp_author <- gsub('\\s+',' ',tmp_author)
    tmp_author <- gsub('Authors:','',tmp_author) %>% str_trim
    author <- c(author, tmp_author)  
    
    # subject
    tmp_subject <- tmp_paragraph %>% html_nodes(???) %>% html_text(T)
    subject <- c(subject, tmp_subject)
    
    # dataLine
    tmp_dataLine <- tmp_paragraph %>% html_nodes(???) %>% html_text(T)
    dataLine <- c(dataLine, tmp_dataLine)
  }
}
final <- ???

# Export the result
write.csv(???, file = "Deep Learning arxiv papers.csv")
```



# Question 4  
#### 4 ) 문제 3번의 코드를 사용하여 'http://movie.naver.com/movie/point/af/list.nhn' 사이트(네이버 영화 평점)에서 과제일 기준 1~3쪽의 리뷰(30개)에 대하여 평점과 영화 제목을 scraping하시오(*내용 제외, 결과물은 'titles&points.csv'로 저장하여 제출할 것.)
```{r}
???
```

